{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook genérico que contém as funções utilizadas nos demais notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções Genéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARÂMETROS\n",
    "\n",
    "#Máquina e porta (formato host:port)\n",
    "SOLR_ADDR='localhost:8983'\n",
    "ELASTIC_ADDR='localhost:9200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "headers = {'content-type': 'application/json;charset=UTF-8'}\n",
    "\n",
    "def date_diff_in_seconds(dt2, dt1):\n",
    "    timedelta = dt2 - dt1\n",
    "    return timedelta.days * 24 * 3600 + timedelta.seconds\n",
    "\n",
    "# Some utilities for flattening the explain into something a bit more\n",
    "# readable. Pass Explain JSON, get readable (ironically this is what Solr's default output is :-p)\n",
    "def flatten(l):\n",
    "    [item for sublist in l for item in sublist]\n",
    "\n",
    "def simplerExplain(explainJson, depth=0):\n",
    "    result = \" \" * (depth * 2) + \"%s, %s\\n\" % (explainJson['value'], explainJson['description'])\n",
    "    #print json.dumps(explainJson, indent=True)\n",
    "    if 'details' in explainJson:\n",
    "        for detail in explainJson['details']:\n",
    "            result += simplerExplain(detail, depth=depth+1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Retorna um dicionário do python com os dados dos filmes\n",
    "def extract():\n",
    "    return pickle.load(open(\"../Dados/movies.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solr: reindexação, pesquisa e explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria um índice novo no Solr e reindexa os dados\n",
    "def reindex_solr(movieDict={}, delete=True):\n",
    "    if delete:\n",
    "        resp = requests.get(\"http://\" + SOLR_ADDR + \"/solr/admin/collections?action=DELETE&name=tmdb\")\n",
    "        resp = requests.get(\"http://\" + SOLR_ADDR + \"/solr/admin/collections?action=CREATE&name=tmdb&numShards=1\")\n",
    "        print(\"solr building...\", resp.status_code)\n",
    "    \n",
    "    movies = \"\"\n",
    "    \n",
    "    for id, movie in movieDict.items():\n",
    "        movies += json.dumps(movie) + \",\"\n",
    "    \n",
    "    bulkMovies = \"[\" + movies + \"]\"\n",
    "\n",
    "    print(\"solr indexing...\")\n",
    "    resp = requests.post(\"http://\" + SOLR_ADDR + \"/solr/tmdb/update/json/docs?commit=true\", data=bulkMovies, headers=headers)\n",
    "    print(\"solr indexing done.\", resp.status_code, resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faz a pesquisa especificada no Solr e imprime os resultados \n",
    "def get_field(hit, name):\n",
    "    if name in hit:\n",
    "        return hit[name]\n",
    "    return ''\n",
    "\n",
    "def search_solr(usersSearch, qf='title overview'):\n",
    "    url = 'http://' + SOLR_ADDR + '/solr/tmdb/select?q='+ usersSearch + '&defType=edismax&qf=' + qf + '&rows=30&wt=json&fl=title,overview,cast.name, directors.name,score'\n",
    "    httpResp = requests.get(url, headers=headers) #A\n",
    "    if httpResp.status_code != 200:\n",
    "        print('Erro ao executar a consulta: ')\n",
    "        print(httpResp.text)\n",
    "        return\n",
    "    searchHits = json.loads(httpResp.text)['response']['docs']\n",
    "    print(\"Solr results\")\n",
    "    lista_resultados = []\n",
    "    for idx, hit in enumerate(searchHits):\n",
    "        filme = [idx + 1, hit['score'], hit['title'], get_field(hit,'overview'), hit['cast.name'], hit['directors.name']]\n",
    "        lista_resultados.append(filme)\n",
    "    \n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    \n",
    "    df = pd.DataFrame(lista_resultados,columns=['Num', 'Relevance Score', 'Movie Title', 'Overview', 'Cast', 'Director'], index=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_solr(users_search, qf='title overview cast.name.bigramed directors.name.bigramed'):\n",
    "    url = 'http://' + SOLR_ADDR + '/solr/tmdb/select?q='+ users_search + '&debugQuery=true&defType=edismax&qf=' + qf +'&rows=1&wt=json&fl=title,score'\n",
    "    httpResp = requests.get(url, headers=headers)\n",
    "    explain = json.loads(httpResp.text)['debug']['parsedquery']\n",
    "    print('Explicação da query no Solr:')\n",
    "    print(explain)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elasticsearch: reindexação, pesquisa e explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria um índice novo no Elastic e reindexa os dados\n",
    "def reindex_elastic(analysisSettings={}, mappingSettings={}, movieDict={}):\n",
    "    start = datetime.now()\n",
    "    \n",
    "    settings = { #A\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1, #B\n",
    "            \"index\": {\n",
    "                \"analysis\" : analysisSettings, #C\n",
    "    }}}\n",
    "\n",
    "    if mappingSettings:\n",
    "        settings['mappings'] = mappingSettings #C\n",
    "\n",
    "    resp = requests.delete(\"http://\" + ELASTIC_ADDR + \"/tmdb\") #D\n",
    "    resp = requests.put(\"http://\" + ELASTIC_ADDR + \"/tmdb\", \n",
    "                        data=json.dumps(settings), headers=headers)\n",
    "\n",
    "    print(\"elastic building...\", resp.status_code)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        print(resp.text)\n",
    "    \n",
    "    bulkMovies = \"\"\n",
    "    \n",
    "    for id, movie in movieDict.items(): \n",
    "        addCmd = {\"index\": {\"_index\": \"tmdb\", #E\n",
    "                            #\"_type\": \"movie\",\n",
    "                            \"_id\": movie['id']}}\n",
    "        bulkMovies += json.dumps(addCmd) + \"\\n\" + json.dumps(movie) + \"\\n\"\n",
    "\n",
    "    print(\"elastic indexing...\")\n",
    "    resp = requests.post(\"http://\" + ELASTIC_ADDR + \"/_bulk\", data=bulkMovies, headers=headers)\n",
    "    print(\"elastic indexing done.\", resp.status_code)\n",
    "    \n",
    "    end = datetime.now()\n",
    "    delta = date_diff_in_seconds(end, start)\n",
    "    print('Elastic done! (took %d seconds)\\n' % (delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cast_list(cast, n):\n",
    "    i = 0\n",
    "    cast_list = []\n",
    "    for e in cast:\n",
    "        i += 1\n",
    "        cast_list.append(e['name'])\n",
    "    return cast_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faz a pesquisa especificada no Elasticsearch e imprime os resultados\n",
    "def search_elastic(usersSearch, query=None):\n",
    "    if not query:\n",
    "        query = {\n",
    "            'query': {\n",
    "                'multi_match': { \n",
    "                    'query': usersSearch, #A\n",
    "                    'fields': ['title^10', 'overview'] #B\n",
    "                }\n",
    "            },\n",
    "            'size': '30'\n",
    "        }\n",
    "    \n",
    "    url = 'http://'+ ELASTIC_ADDR +'/tmdb/_search'\n",
    "    httpResp = requests.get(url, data=json.dumps(query), headers=headers) #A\n",
    "    searchHits = json.loads(httpResp.text)['hits']\n",
    "    print(\"Elasticsearch results\")\n",
    "    lista_resultados = []\n",
    "    for idx, hit in enumerate(searchHits['hits']):\n",
    "        filme = [idx + 1, hit['_score'], hit['_source']['title'], hit['_source']['overview'], get_cast_list(hit['_source']['cast'],10), get_cast_list(hit['_source']['directors'],5)]\n",
    "        lista_resultados.append(filme)\n",
    "    \n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    \n",
    "    df = pd.DataFrame(lista_resultados,columns=['Num', 'Relevance Score', 'Movie Title', 'Overview', 'Cast', 'Director'], index=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_elastic(query):\n",
    "    httpResp = requests.get('http://'+ ELASTIC_ADDR +'/tmdb/_validate/query?explain',data=json.dumps(query), headers=headers)\n",
    "    print('Explicação da query no Elasticsearch:')\n",
    "    json_str= json.dumps(json.loads(httpResp.text), indent=2, ensure_ascii=False).encode('utf-8')\n",
    "    print(json_str.decode())\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
